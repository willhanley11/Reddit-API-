**Project Summary** 

This project used a binary classification model to differentiate posts from two different subreddits: r/TheOnion and r/NotTheOnion. While I expect most are familiar with the satirical news site, The Onion, r/NotTheOnion contains news that seems like it could be from The Onion, but in fact is actually truthful news. The goal was to create a model that would read a post from one of the two subreddits, and be able to tell from which subreddit it came. It has been said that computers are not a good judge for sarcasm, so I was eager to find out if that was actually the case. 

The first step towards achieving my goal was to gather the data by using  the requests library and pull from the Reddit API. A problem I ran into was that the maximum allowance of submissions I could get in one request was just 100. To have a dataframe with such little data wouldn’t serve me much good, so I was able to create a function that would surpass this limit. A unique feature of the function was the use of the ‘epoch time’ of each post. The Epoch time is the number of seconds past since January 1, 1970 and each post had this distinct timestamp. Using this timestamp, I was able to create the next request of 100 submissions starting at the last request epoch time. All in all, I created a cleaned (mainly just dropped rows with a few specific proper nouns) data frame with 5,000 submissions from r/TheOnion and 5,000 submissions from r/NotTheOnion. 

For the modeling process, I chose to use two different transformers on the data frame. The first was a basic countvectorizer, which counted all the times each word was found in the data frame. The second transformer (TFIDF) used the same method as the countvectorizer, but also added weights to words it found to be unique. Four different estimators were then used, along with a gridsearch for each, to see which model performed the best on a train-test split of the dataframe. While the first three models all performed rather equally, the final model, a support vector classifier, had the highest accuracy score on the test split of the data. The model ended up being able to differentiate posts at a very impressive 87.4% accuracy rate (1,943 out of 2,222 submissions correctly predicted). 

While this project was primarily just to see if I could differentiate between two subreddits, the results show just how powerful transformers and estimators actually are when sorting through text data. I am excited with the success of my results, and equally eager to use my modeling process in a more practical manner. 


